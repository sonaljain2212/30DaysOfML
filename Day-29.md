# #30dayoftechreading

Day 29/30

Quote of the day: Sometime all we need is attention!! :P

Just a day more to go!! I came a long way already. I chose to read another paper on Natural Language Processing titled "Attention is all you need". This paper propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism.
